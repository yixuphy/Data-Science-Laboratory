{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping, Entropy and ICML papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICML - the International Conference on Machine Learning - is a top research conference in Machine learning. Scrape all the pdfs of all ICML 2019 papers from http://proceedings.mlr.press/v97/.\n",
    "1. What are the top 10 common words in the ICML papers?\n",
    "2. Let $Z$ be a randomly selected word in a randomly selected ICML paper. Estimate the entropy of $Z$.\n",
    "3. Synthesize a random paragraph using the marginal distribution over words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the website, get the links of the pdfs and download the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to website and get list of all pdfs\n",
    "url='http://proceedings.mlr.press/v97/'\n",
    "response = request.urlopen(url).read()\n",
    "soup= BeautifulSoup(response, \"html.parser\")     \n",
    "links = soup.find_all('a', href=re.compile(r'(.pdf)')) #find the links .pdf in the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the pdf link names\n",
    "url_list = []\n",
    "for el in links:\n",
    "    url_list.append((el['href'])) #The url of the pdfs are in ['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_location = r'C:\\Users\\yixup\\Desktop\\ee_460j\\lab5\\webscraping'\n",
    "if not os.path.exists(folder_location):os.mkdir(folder_location) #If the folder doesn't exist, create a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the pdfs to a specified location\n",
    "for url in url_list:\n",
    "    #Name the pdfs according to the link \n",
    "    fullfilename = os.path.join(folder_location, url.replace('http://proceedings.mlr.press/v97/', '').replace('/', '_'))\n",
    "    request.urlretrieve(url, fullfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all the text from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "from tika import parser\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "for filename in glob.glob(os.path.join(folder_location, '*.pdf')): # find all the files ended with .pdf in the folder \n",
    "    #key = filename.replace(folder_location, '').replace('.pdf', '')[1:]\n",
    "    raw = parser.from_file(filename) #Get the content, title and other information of the pdfs\n",
    "    regex = '[a-zA-Z]' #the form is a-z, A-Z\n",
    "    if raw['content']==None:\n",
    "        continue\n",
    "    \n",
    "    #The word_tokenize() function will break our text phrases into #individual words\n",
    "    tokens = word_tokenize(raw['content'])\n",
    "    #Create a list with word in the form of the regex and longer than 1\n",
    "    keyword = [word for word in tokens if not len(word) <= 1 and not re.search(regex, word) == None]\n",
    "    keywords.extend(keyword) #A long list with all the words from all files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the most common 10 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter #A way to get the top N common elements in the list fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without filtering all the common English words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 369109),\n",
       " ('of', 201161),\n",
       " ('and', 179808),\n",
       " ('to', 121539),\n",
       " ('is', 106814),\n",
       " ('in', 102706),\n",
       " ('for', 89356),\n",
       " ('that', 71473),\n",
       " ('we', 68803),\n",
       " ('with', 60573)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c0 = Counter(keywords)\n",
    "print('Without filtering all the common English words')\n",
    "c0.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The words above cannot give us any useful information, therefore, we need to do some filtering below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all the common words out e.g. 'a', 'an', 'we', et cl.\n",
    "stop_words = stopwords.words('english')\n",
    "update1 = ['et', 'The', 'In', 'We', 'al.', 'For', 'using', 'M.', 'A.', 'S.', 'Figure', 'J.', 'use', 'pp', 'also', 'This',\n",
    "          'D.', 'al', 'one', 'two', 'first', 'R.', 'Let', 'i=1', 'To', 'used', 'C.', 'P.', 'T.', 'K.', 'Then', 'based', 'L.',\n",
    "          'ing', 'G.', 'Our', 'Proof', 'Section', 'Y.', 'Table', 'As', 'xi', 'via', 'H.', 'It', 'However', 'example', 'https',\n",
    "          'graph', 'may', 'since', 'E.', 'N.', 'B.', 'xt', 'i.e.', 'Since', 'By', 'On', 'Thus', 'let', 'Eq', 'even', 'F.',\n",
    "          'Appendix', 'us', 't=1', 'I.', 'Rd', 'three', 'Li', 'V.', 'W.', 'respectively', 'n∑', 'thus', 'j=1', 'fact', 'An',\n",
    "          'e.g.', 'known', 'note', 'xk', 'ln', 'st', 'Here', 'x1', 'Z.', 'Finally', 'graphs', 'These', 'Fig', 'Pr', 'tions',\n",
    "          'instead', '/latexit', 'latexit', 're-', 'sha1_base64=', 'First', 'RL', 'like', 'real', 'com-', 'while', 'e.g.', 'T∑',\n",
    "          'often', 'X.', 'yi', 'gives', 'within', 'x*', 'At', 'If', 'Given', 'still', 'per', 'e.g', 'in-', 'x∗', 'x0', 's′',\n",
    "          'One', 't+1', 'th', 'de-', 'O.', 'wt']\n",
    "stop_words.extend(update1)\n",
    "keywords_filtered = [word for word in keywords if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaningful words after filtering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('learning', 18821),\n",
       " ('model', 17814),\n",
       " ('data', 15188),\n",
       " ('set', 14700),\n",
       " ('function', 14483),\n",
       " ('log', 13214),\n",
       " ('Learning', 12412),\n",
       " ('training', 11866),\n",
       " ('algorithm', 11459),\n",
       " ('distribution', 9601)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = Counter(keywords_filtered)\n",
    "print('Meaningful words after filtering')\n",
    "c1.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. To estimate the entropy\n",
    "use the top 10000 elements since when p is close to 0, the corresponding entropy is low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate an single term in entropy\n",
    "def entropy(p):\n",
    "    return -p * np.log2(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calcualte the estimated entropy of a randomly selected variable Z\n",
    "def entropy_tot(key, n_estimate=10000):\n",
    "    \n",
    "    keycopy = key.copy()\n",
    "    c = Counter(keycopy)\n",
    "    n_tot = len(keycopy)\n",
    "    \n",
    "    #n_estimate is the num. chosen to estimate the entropy (the larger this number is, the closer it is to the real value)\n",
    "    top = c.most_common(n_estimate)\n",
    "    \n",
    "    #Vectorize top in order to faciliate the computation: translate to an numpy array; all the elements are str, but we want \n",
    "    #the second column to be int, do another transformation\n",
    "    top_result = np.array(top)\n",
    "    p = top_result[:, 1].astype(int) / n_tot #get the probability of each word (top n_estimate words)\n",
    "    \n",
    "    #Vectorize the map of entropy\n",
    "    map_entropy = lambda x : entropy(x)\n",
    "    v_map_entropy = np.vectorize(map_entropy)\n",
    "    entropy_array = v_map_entropy(p)\n",
    "    \n",
    "    return np.sum(entropy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated entropy is:  9.140278196305813\n"
     ]
    }
   ],
   "source": [
    "print('Estimated entropy is: ', entropy_tot(keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Synthesize a random paragraph using the marginal distribution over words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = c0.most_common(100)\n",
    "#Vectorize top in order to faciliate the computation and get a list with all the probability in the p_values 1d numpy array\n",
    "top_result = np.array(top)\n",
    "p_values = top_result[:, 1].astype(int) / np.sum(top_result[:, 1].astype(int)) # Normalization\n",
    "word_list = top_result[:, 0] # 1d numpy array have the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the same time not that in algorithm In data Theorem and if the and the al the in the that of problem of with is For to of .\n",
      "as log of the an Figure the of are gradient that for .\n",
      "the et from bound which where that and the to for the .\n",
      "in be and The and as on be the with the network in if .\n",
      "as are by problem Figure be is of same not in method an the .\n",
      "in also case which the different when loss in the where The with by data are different in use then by time is networks on for to .\n",
      "be which of the of the with of The of data The and it .\n",
      "have are of case In to al in of our Let set if we .\n",
      "et the also the of learning is for of loss it of to an .\n",
      "used number for the using we Learning We and of and In in for .\n",
      "with in has is al is of is the on and of .\n",
      "problem only we that to we the as log the from using results for .\n",
      "The that are with the in that has log the pp and the of for .\n",
      "of for The of and the to algorithm of for then the for .\n",
      "which not our of or Conference is also to the is of over of al .\n",
      "we Learning such the with which and function are for the and we .\n",
      "is of with our is we model al have et by of to in .\n",
      "the in the algorithm with the to as with that with these of on it "
     ]
    }
   ],
   "source": [
    "# Generate a random paragraph (300 words) with weighted values\n",
    "previous = ''\n",
    "for i in range (300):\n",
    "    word = str(np.random.choice(word_list, 1, p=p_values))\n",
    "    w = re.sub(r'[\\W]', '', word)\n",
    "    \n",
    "    if previous==w or len(w)==1:\n",
    "        continue\n",
    "        \n",
    "    if (i%15 == 0) and (i > 15):\n",
    "        print('.')\n",
    "        \n",
    "    print(w, end=' ')\n",
    "    \n",
    "    previous = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
